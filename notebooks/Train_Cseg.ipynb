{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml pandas scikit-learn albumentations segmentation-models-pytorch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, best_val_dice, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_val_dice\": best_val_dice\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"ğŸ’¾ Checkpoint saved at {path} (Epoch {epoch+1}, Dice {best_val_dice:.4f})\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1  # resume from next epoch\n",
    "    best_val_dice = checkpoint[\"best_val_dice\"]\n",
    "    print(f\"âœ… Resumed from checkpoint at epoch {start_epoch} with best Dice {best_val_dice:.4f}\")\n",
    "    return model, optimizer, start_epoch, best_val_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded for 512x512 Cseg training.\n",
      "{'IMAGE_DIR': '/content/drive/MyDrive/CAF-GAN/mimic-cxr-jpg-2.0.0/files/', 'MASK_DIR': '/content/drive/MyDrive/CAF-GAN/data/masks_512x512/', 'TRAIN_CSV_PATH': '/content/drive/MyDrive/CAF-GAN/data/splits/train.csv', 'VAL_CSV_PATH': '/content/drive/MyDrive/CAF-GAN/data/splits/val.csv', 'OUTPUT_DIR': '/content/drive/MyDrive/CAF-GAN/outputs/cseg_512/', 'MODEL_NAME': 'best_cseg_512.pth', 'IMG_SIZE': 512, 'BATCH_SIZE': 8, 'EPOCHS': 25, 'LEARNING_RATE': 0.0001, 'DEVICE': 'cuda', 'NUM_WORKERS': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7a4b91b42e4f5c9b79b0a4bd9d121e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b321eec8c6c24403831a31f68c453481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Resumed from checkpoint at epoch 22 with best Dice 0.9525\n",
      "\n",
      "ğŸ‹ï¸â€â™€ï¸ Training Cseg model with 512x512 images...\n",
      "\n",
      "--- Epoch 23/25 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [26:31<00:00,  9.09s/it]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [05:36<00:00,  8.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6631 | Val Loss: 0.6670 | Val Dice Score: 0.9508\n",
      "ğŸ’¾ Checkpoint saved at /content/drive/MyDrive/CAF-GAN/outputs/cseg_512/checkpoint.pth (Epoch 23, Dice 0.9525)\n",
      "\n",
      "--- Epoch 24/25 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [03:42<00:00,  1.27s/it]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:42<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6612 | Val Loss: 0.6681 | Val Dice Score: 0.9502\n",
      "ğŸ’¾ Checkpoint saved at /content/drive/MyDrive/CAF-GAN/outputs/cseg_512/checkpoint.pth (Epoch 24, Dice 0.9525)\n",
      "\n",
      "--- Epoch 25/25 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [03:41<00:00,  1.26s/it]\n",
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:42<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6618 | Val Loss: 0.6674 | Val Dice Score: 0.9501\n",
      "ğŸ’¾ Checkpoint saved at /content/drive/MyDrive/CAF-GAN/outputs/cseg_512/checkpoint.pth (Epoch 25, Dice 0.9525)\n",
      "\n",
      "âœ… Training complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "CONFIG = {\n",
    "    \"IMAGE_DIR\": \"/content/drive/MyDrive/CAF-GAN/mimic-cxr-jpg-2.0.0/files/\",\n",
    "    \"MASK_DIR\": \"/content/drive/MyDrive/CAF-GAN/data/masks_512x512/\",\n",
    "    \"TRAIN_CSV_PATH\": \"/content/drive/MyDrive/CAF-GAN/data/splits/train.csv\",\n",
    "    \"VAL_CSV_PATH\": \"/content/drive/MyDrive/CAF-GAN/data/splits/val.csv\",\n",
    "    \"OUTPUT_DIR\": \"/content/drive/MyDrive/CAF-GAN/outputs/cseg_512/\",\n",
    "    \"MODEL_NAME\": \"best_cseg_512.pth\",\n",
    "    \"IMG_SIZE\": 512,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"EPOCHS\": 25,\n",
    "    \"LEARNING_RATE\": 0.0001,\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"NUM_WORKERS\": 2\n",
    "}\n",
    "print(\"âœ… Configuration loaded for 512x512 Cseg training.\")\n",
    "print(CONFIG)\n",
    "\n",
    "# --- 2. PyTorch Dataset (MODIFIED) ---\n",
    "class MIMICXRSegmentationDataset(Dataset):\n",
    "    # We pass img_size to the constructor\n",
    "    def __init__(self, df, image_dir, mask_dir, img_size, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_size = img_size # <-- Store image size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        subject_id, study_id, dicom_id = str(row['subject_id']), str(row['study_id']), row['dicom_id']\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, f'p{subject_id[:2]}', f'p{subject_id}', f's{study_id}', f'{dicom_id}.jpg')\n",
    "        mask_path = os.path.join(self.mask_dir, f\"{dicom_id}.png\")\n",
    "\n",
    "        # Load high-resolution image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # --- KEY CHANGE: Resize the image to match the mask size BEFORE augmentation ---\n",
    "        # Use high-quality LANCZOS resampling\n",
    "        image = image.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        image = np.array(image)\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "        # Load the 512x512 mask\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "\n",
    "        # Normalize mask to 0.0-1.0 range\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        # Now, both image and mask are 512x512, so the transform will work\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.unsqueeze(0)\n",
    "\n",
    "# --- 3. Loss, Metrics & Training Functions (Unchanged) ---\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        inputs_flat, targets_flat = inputs.view(-1), targets.view(-1)\n",
    "        intersection = (inputs_flat * targets_flat).sum()\n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (inputs_flat.sum() + targets_flat.sum() + smooth)\n",
    "        bce_loss = nn.BCEWithLogitsLoss()(inputs, targets)\n",
    "        return bce_loss + dice_loss\n",
    "\n",
    "def dice_score(preds, targets, smooth=1e-6):\n",
    "    preds = torch.sigmoid(preds) > 0.5\n",
    "    preds_flat, targets_flat = preds.float().view(-1), targets.view(-1)\n",
    "    intersection = (preds_flat * targets_flat).sum()\n",
    "    return (2. * intersection + smooth) / (preds_flat.sum() + targets_flat.sum() + smooth)\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, total_dice = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Validating\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total_dice += dice_score(outputs, masks).item()\n",
    "    return running_loss / len(dataloader.dataset), total_dice / len(dataloader)\n",
    "\n",
    "# --- 4. Main Training Execution ---\n",
    "def run_training():\n",
    "    DEVICE = CONFIG['DEVICE']\n",
    "    os.makedirs(CONFIG['OUTPUT_DIR'], exist_ok=True)\n",
    "\n",
    "    # The Albumentations transform no longer needs a Resize at the start,\n",
    "    # but it is harmless to keep it as a safeguard.\n",
    "    transform = A.Compose([\n",
    "        A.Resize(CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.7),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.GaussNoise(p=0.2),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    train_df = pd.read_csv(CONFIG['TRAIN_CSV_PATH'])\n",
    "    val_df = pd.read_csv(CONFIG['VAL_CSV_PATH'])\n",
    "\n",
    "    # --- KEY CHANGE: Pass IMG_SIZE to the Dataset constructor ---\n",
    "    train_dataset = MIMICXRSegmentationDataset(train_df, CONFIG['IMAGE_DIR'], CONFIG['MASK_DIR'], CONFIG['IMG_SIZE'], transform)\n",
    "    val_dataset = MIMICXRSegmentationDataset(val_df, CONFIG['IMAGE_DIR'], CONFIG['MASK_DIR'], CONFIG['IMG_SIZE'], transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=CONFIG['NUM_WORKERS'])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=CONFIG['NUM_WORKERS'])\n",
    "\n",
    "    model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=1).to(DEVICE)\n",
    "    criterion = DiceBCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "\n",
    "    # ğŸ”¹ Check if checkpoint exists\n",
    "    checkpoint_path = os.path.join(CONFIG['OUTPUT_DIR'], \"checkpoint.pth\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model, optimizer, start_epoch, best_val_dice = load_checkpoint(model, optimizer, checkpoint_path, DEVICE)\n",
    "    else:\n",
    "        start_epoch, best_val_dice = 0, 0.0\n",
    "        print(\"ğŸš€ Starting training from scratch\")\n",
    "\n",
    "    print(\"\\nğŸ‹ï¸â€â™€ï¸ Training Cseg model with 512x512 images...\")\n",
    "    for epoch in range(start_epoch, CONFIG['EPOCHS']):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{CONFIG['EPOCHS']} ---\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        val_loss, val_dice = validate(model, val_loader, criterion, DEVICE)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice Score: {val_dice:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_dice > best_val_dice:\n",
    "            best_val_dice = val_dice\n",
    "            model_path = os.path.join(CONFIG['OUTPUT_DIR'], CONFIG['MODEL_NAME'])\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"âœ¨ Best model saved to {model_path} (Dice: {val_dice:.4f})\")\n",
    "\n",
    "        # ğŸ”¹ Always save checkpoint (so you can resume tomorrow)\n",
    "        save_checkpoint(model, optimizer, epoch, best_val_dice, checkpoint_path)\n",
    "\n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "\n",
    "run_training()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
