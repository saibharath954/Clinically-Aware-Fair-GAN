{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrhYk2bxB_Yx",
        "outputId": "d743e048-3ce9-43a5-fa62-eb7790425b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d8VZ0P_1C4Kq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b517a4-a722-46c2-f2c4-ebb8e960fa72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pyyaml pandas scikit-learn albumentations segmentation-models-pytorch -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4fKBfR1xUI1",
        "outputId": "3e93b079-b251-43ba-e444-947dc8b128ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tZuZHWYztl1U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------------------\n",
        "# CONFIG\n",
        "# ---------------------------\n",
        "CONFIG = {\n",
        "    \"IMAGE_DIR\": \"/content/drive/MyDrive/CAF-GAN/mimic-cxr-jpg-2.0.0/files/\",\n",
        "    \"TRAIN_CSV_PATH\": \"/content/drive/MyDrive/CAF-GAN/data/splits/train.csv\",\n",
        "    \"CDIAG_PATH\": \"/content/drive/MyDrive/CAF-GAN/outputs/cdiag_512/best_cdiag_512.pth\",\n",
        "    \"CSEG_PATH\": \"/content/drive/MyDrive/CAF-GAN/outputs/cseg_512/best_cseg_512.pth\",\n",
        "    \"OUTPUT_DIR\": \"/content/drive/MyDrive/CAF-GAN/outputs/caf_gan_final/\",\n",
        "    \"IMAGE_OUTPUT_DIR\": \"/content/drive/MyDrive/CAF-GAN/outputs/caf_gan_final/generated_images/\",\n",
        "    \"TARGET_IMG_SIZE\": 512,\n",
        "    \"LATENT_DIM\": 512,\n",
        "    \"CHANNELS\": 3,\n",
        "    \"PROGRESSIVE_EPOCHS\": [20, 20, 20, 40, 40, 40, 100],  # 7 entries for 8..512\n",
        "    \"BATCH_SIZES\": [8, 8, 8, 8, 8, 8, 4],\n",
        "    \"G_LR\": 1e-3,\n",
        "    \"D_LR\": 1e-3,\n",
        "    \"B1\": 0.0,\n",
        "    \"B2\": 0.99,\n",
        "    \"R1_REG_FREQ\": 16,\n",
        "    \"LAMBDA_R1\": 10,\n",
        "    \"LAMBDA_CLINIC\": 0.5,  ### ROBUSTNESS: Tuned weight\n",
        "    \"LAMBDA_FAIR\": 0.2,   ### ROBUSTNESS: Tuned weight\n",
        "\n",
        "    # --- üí° NEW UTILITY LOSS (FROM EVALUATION FEEDBACK) ---\\n\",\n",
        "    \"LAMBDA_UTILITY\": 0.3, # Pushes generator to make diagnosable Pneumonia\"\n",
        "\n",
        "    \"LOSS_ANNEAL_START_EPOCH\": 15,\n",
        "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"NUM_WORKERS\": 2,\n",
        "    \"LOG_FREQ\": 100,\n",
        "    \"SAVE_IMG_FREQ\": 1,\n",
        "    \"CHECKPOINT_FREQ\": 500,\n",
        "\n",
        "    ### ROBUSTNESS FIX: Parameters for new L_clinical and L_fairness\n",
        "    \"CLINICAL_LOSS_TYPE\": \"area\", # 'area' implements L_area\n",
        "    \"PLAUSIBLE_LUNG_AREA_MEAN\": 0.220646, # Mean area of lungs as % of image (NEEDS TO BE PRE-CALCULATED from your Cseg on real data)\n",
        "    \"PLAUSIBLE_LUNG_AREA_STD\": 0.066277, # Std dev of lung area (NEEDS TO BE PRE-CALCULATED)\n",
        "    \"FAIRNESS_BATCH_SIZE\": 30, # Large, stable batch size for fairness loss\n",
        "    \"NUM_RACE_GROUPS\": 5, # From your dataset (White, Black, Asian, Hispanic, Other)\n",
        "\n",
        "    # --- üí° NEW FAIRNESS PUSH (FROM EVALUATION FEEDBACK) ---\",\n",
        "    \"FAIRNESS_PUSH_WEIGHT\": 0.5 # Balances std(TPR) vs. (1 - mean(TPR))\\n\",\n",
        "}\n",
        "\n",
        "os.makedirs(CONFIG['IMAGE_OUTPUT_DIR'], exist_ok=True)\n",
        "os.makedirs(CONFIG['OUTPUT_DIR'], exist_ok=True)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Basic building blocks\n",
        "# (PixelNorm, WSConv2d, InjectNoise, AdaIN)\n",
        "# ---------------------------\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1e-8\n",
        "    def forward(self, x):\n",
        "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
        "\n",
        "class WSConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n",
        "        self.bias = self.conv.bias\n",
        "        self.conv.bias = None\n",
        "        nn.init.normal_(self.conv.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x * self.scale)\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
        "        return out\n",
        "\n",
        "class InjectNoise(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
        "    def forward(self, x):\n",
        "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device=x.device)\n",
        "        return x + self.weight * noise\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    def __init__(self, channels, w_dim):\n",
        "        super().__init__()\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
        "        self.style_scale = nn.Linear(w_dim, channels)\n",
        "        self.style_bias = nn.Linear(w_dim, channels)\n",
        "    def forward(self, x, w):\n",
        "        x = self.instance_norm(x)\n",
        "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
        "        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
        "        return style_scale * x + style_bias\n",
        "\n",
        "# ---------------------------\n",
        "# Mapping & Generator blocks\n",
        "# (MappingNetwork, GenBlock, Generator)\n",
        "# ---------------------------\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, z_dim, w_dim):\n",
        "        super().__init__()\n",
        "        layers = [PixelNorm()]\n",
        "        for i in range(8):\n",
        "            if i == 0:\n",
        "                layers.append(nn.Linear(z_dim, w_dim))\n",
        "            else:\n",
        "                layers.append(nn.Linear(w_dim, w_dim))\n",
        "            if i < 7:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.mapping = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.mapping(x)\n",
        "\n",
        "class GenBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, w_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
        "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.inject_noise1 = InjectNoise(out_channels)\n",
        "        self.inject_noise2 = InjectNoise(out_channels)\n",
        "        self.adain1 = AdaIN(out_channels, w_dim)\n",
        "        self.adain2 = AdaIN(out_channels, w_dim)\n",
        "    def forward(self, x, w):\n",
        "        x = self.leaky(self.inject_noise1(self.conv1(x)))\n",
        "        x = self.adain1(x, w)\n",
        "        x = self.leaky(self.inject_noise2(self.conv2(x)))\n",
        "        x = self.adain2(x, w)\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, w_dim, base_channels, img_channels=3):\n",
        "        super().__init__()\n",
        "        self.starting_const = nn.Parameter(torch.randn(1, base_channels, 4, 4))\n",
        "        self.map = MappingNetwork(z_dim, w_dim)\n",
        "        self.initial_conv = WSConv2d(base_channels, base_channels, kernel_size=3, padding=1)\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.factors = [512, 512, 512, 256, 128, 64, 32, 16]\n",
        "        assert base_channels == self.factors[0]\n",
        "        self.prog_blocks = nn.ModuleList()\n",
        "        self.to_rgbs = nn.ModuleList()\n",
        "        self.to_rgbs.append(WSConv2d(self.factors[0], img_channels, kernel_size=1, padding=0))\n",
        "        for i in range(1, len(self.factors)):\n",
        "            in_c = self.factors[i-1]\n",
        "            out_c = self.factors[i]\n",
        "            self.prog_blocks.append(GenBlock(in_c, out_c, w_dim))\n",
        "            self.to_rgbs.append(WSConv2d(out_c, img_channels, kernel_size=1, padding=0))\n",
        "\n",
        "    def forward(self, z, alpha, steps):\n",
        "        w = self.map(z)\n",
        "        batch = z.shape[0]\n",
        "        x = self.starting_const.repeat(batch, 1, 1, 1)\n",
        "        x = self.initial_conv(x)\n",
        "        x = self.leaky(x)\n",
        "        if steps == 0:\n",
        "            out = self.to_rgbs[0](x)\n",
        "            return torch.tanh(out)\n",
        "        prev = None\n",
        "        for step in range(1, steps + 1):\n",
        "            prev = x\n",
        "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "            x = self.prog_blocks[step - 1](x, w)\n",
        "        final_out = self.to_rgbs[steps](x)\n",
        "        if alpha < 1.0 and prev is not None:\n",
        "            prev_rgb = self.to_rgbs[steps - 1](prev)\n",
        "            prev_rgb_upsampled = F.interpolate(prev_rgb, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "            out = alpha * final_out + (1.0 - alpha) * prev_rgb_upsampled\n",
        "        else:\n",
        "            out = final_out\n",
        "        return torch.tanh(out)\n",
        "\n",
        "# ---------------------------\n",
        "# Discriminator\n",
        "# (DiscBlock, Discriminator)\n",
        "# ---------------------------\n",
        "class DiscBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = WSConv2d(in_channels, in_channels)\n",
        "        self.conv2 = WSConv2d(in_channels, out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
        "    def forward(self, x):\n",
        "        x = self.leaky(self.conv1(x))\n",
        "        x = self.leaky(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, base_channels, img_channels=3):\n",
        "        super().__init__()\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.factors = [512, 512, 512, 256, 128, 64, 32, 16]\n",
        "        assert base_channels == self.factors[0]\n",
        "        self.from_rgbs = nn.ModuleList([WSConv2d(img_channels, ch, kernel_size=1, padding=0) for ch in self.factors])\n",
        "        self.prog_blocks = nn.ModuleList()\n",
        "        for i in range(1, len(self.factors)):\n",
        "            in_c = self.factors[i]\n",
        "            out_c = self.factors[i-1]\n",
        "            self.prog_blocks.append(DiscBlock(in_c, out_c))\n",
        "        self.final_block = nn.Sequential(\n",
        "            WSConv2d(self.factors[0] + 1, self.factors[0]), self.leaky,\n",
        "            WSConv2d(self.factors[0], self.factors[0], kernel_size=4, padding=0, stride=1), self.leaky,\n",
        "            WSConv2d(self.factors[0], 1, kernel_size=1, padding=0, stride=1)\n",
        "        )\n",
        "    def forward(self, x, alpha, steps):\n",
        "        out = self.leaky(self.from_rgbs[steps](x))\n",
        "        for s in range(steps, 0, -1):\n",
        "            out = self.prog_blocks[s - 1](out)\n",
        "            out = F.avg_pool2d(out, kernel_size=2)\n",
        "            if alpha < 1.0 and s == steps:\n",
        "                downscaled_image = F.avg_pool2d(x, kernel_size=2)\n",
        "                low_res_features = self.leaky(self.from_rgbs[s - 1](downscaled_image))\n",
        "                out = alpha * out + (1.0 - alpha) * low_res_features\n",
        "        std = torch.std(out, dim=0, unbiased=False).mean().view(1, 1, 1, 1)\n",
        "        std_map = std.repeat(out.shape[0], 1, out.shape[2], out.shape[3])\n",
        "        out = torch.cat([out, std_map], dim=1)\n",
        "        out = self.final_block(out)\n",
        "        return out.view(out.shape[0], -1)\n",
        "\n",
        "# --------------------\n",
        "# Dataset & utilities\n",
        "# --------------------\n",
        "class MIMIC_GAN_Dataset(Dataset):\n",
        "    def __init__(self, df, image_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.df['race_group'] = self.df['race_group'].astype('category')\n",
        "        self.one_hot_races = pd.get_dummies(self.df['race_group']).reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        subject_id = str(row['subject_id'])\n",
        "        study_id = str(row['study_id'])\n",
        "        dicom_id = row['dicom_id']\n",
        "        image_path = os.path.join(self.image_dir, f'p{subject_id[:2]}', f'p{subject_id}', f's{study_id}', f'{dicom_id}.jpg')\n",
        "        try:\n",
        "            image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "        except Exception:\n",
        "            return None, None, None\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "        label = torch.tensor(row['Pneumonia'], dtype=torch.float32)\n",
        "        race = torch.tensor(self.one_hot_races.iloc[idx].values, dtype=torch.float32)\n",
        "        return image, label, race\n",
        "\n",
        "def custom_collate(batch):\n",
        "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "    if not batch:\n",
        "        return torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "def load_critics(cdiag_path, cseg_path, device):\n",
        "    print(\"üß† Loading frozen critic models...\")\n",
        "    Cdiag = models.resnet50()\n",
        "    Cdiag.fc = nn.Linear(Cdiag.fc.in_features, 1)\n",
        "    Cdiag.load_state_dict(torch.load(cdiag_path, map_location=device))\n",
        "    Cdiag.eval().to(device)\n",
        "    for param in Cdiag.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"   - Cdiag (Diagnostic Critic) loaded.\")\n",
        "\n",
        "    Cseg = smp.Unet(\"resnet34\", in_channels=3, classes=1)\n",
        "    Cseg.load_state_dict(torch.load(cseg_path, map_location=device))\n",
        "    Cseg.eval().to(device)\n",
        "    for param in Cseg.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"   - Cseg (Segmentation Critic) loaded.\")\n",
        "    return Cdiag, Cseg\n",
        "\n",
        "def save_checkpoint(gen, disc, opt_gen, opt_disc, epoch, step, alpha, path):\n",
        "    state = {\n",
        "        'gen': gen.state_dict(), 'disc': disc.state_dict(),\n",
        "        'opt_gen': opt_gen.state_dict(), 'opt_disc': opt_disc.state_dict(),\n",
        "        'epoch': epoch, 'step': step, 'alpha': alpha,\n",
        "    }\n",
        "    torch.save(state, path)\n",
        "\n",
        "def load_checkpoint(gen, disc, opt_gen, opt_disc, path, device):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"‚úÖ Resuming training from checkpoint: {path}\")\n",
        "        state = torch.load(path, map_location=device)\n",
        "        gen.load_state_dict(state['gen'])\n",
        "        disc.load_state_dict(state['disc'])\n",
        "        opt_gen.load_state_dict(state['opt_gen'])\n",
        "        opt_disc.load_state_dict(state['opt_disc'])\n",
        "        return state['epoch'], state['step'], state['alpha']\n",
        "    print(\"üì≠ No checkpoint found, starting from scratch.\")\n",
        "    return 0, 0, 1.0\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "### ROBUSTNESS FIX: New Clinical Loss ###\n",
        "# ----------------------------------------------------\n",
        "def calculate_clinical_loss(fake_masks, config):\n",
        "    \"\"\"\n",
        "    Calculates the L_clinical loss based.\n",
        "    This version implements L_area.\n",
        "    \"\"\"\n",
        "    if config[\"CLINICAL_LOSS_TYPE\"] == \"area\":\n",
        "        # fake_masks are logits (B, 1, H, W)\n",
        "        fake_masks_prob = torch.sigmoid(fake_masks)\n",
        "\n",
        "        # Calculate the area of the mask as a percentage of total pixels\n",
        "        # (B, 1, H, W) -> (B,)\n",
        "        total_pixels = fake_masks_prob.shape[2] * fake_masks_prob.shape[3]\n",
        "        mask_area_percent = fake_masks_prob.sum(dim=[1, 2, 3]) / total_pixels\n",
        "\n",
        "        # Get target plausible area stats\n",
        "        mean_area = config[\"PLAUSIBLE_LUNG_AREA_MEAN\"]\n",
        "        std_area = config[\"PLAUSIBLE_LUNG_AREA_STD\"]\n",
        "\n",
        "        # Calculate L1 distance from the mean, normalized by std\n",
        "        # This penalizes masks that are too large or too small\n",
        "        area_loss = F.l1_loss(mask_area_percent, torch.tensor(mean_area, device=mask_area_percent.device)) / std_area\n",
        "        return area_loss\n",
        "    else:\n",
        "        # Placeholder for L_contiguity or L_shape\n",
        "        return torch.tensor(0.0, device=fake_masks.device)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# --- üí° NEW UTILITY LOSS FUNCTION (FROM EVALUATION FEEDBACK) ---\n",
        "# --------------------------------------------------------------\n",
        "def calculate_diag_utility_loss(fake_images_512, cdiag):\n",
        "    \"\"\"\n",
        "    Calculates the utility loss.\n",
        "    This pushes the generator to create images that Cdiag sees as 'Pneumonia'.\n",
        "    \"\"\"\n",
        "    # Get predictions (logits) from Cdiag for the *in-batch* fakes\n",
        "    diag_preds_logits = cdiag(fake_images_512).squeeze()\n",
        "\n",
        "    # Create target labels of all '1.0' (Pneumonia)\n",
        "    target_labels = torch.ones_like(diag_preds_logits)\n",
        "\n",
        "    # Calculate the loss: how far are the predictions from 1.0?\n",
        "    loss = F.binary_cross_entropy_with_logits(diag_preds_logits, target_labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- üí° UPGRADED FAIRNESS LOSS (FROM EVALUATION FEEDBACK) ---\n",
        "# ----------------------------------------------------------------\n",
        "def calculate_fairness_loss(gen, cdiag, alpha, steps, config):\n",
        "    \"\"\"\n",
        "    Calculates a robust L_fairness.\n",
        "    Now combines two objectives:\n",
        "    1. loss_std: Make all TPRs EQUAL (std -> 0)\n",
        "    2. loss_push: Make all TPRs HIGH (mean -> 1.0)\n",
        "    \"\"\"\n",
        "    gen.eval() # Use eval mode for this calculation\n",
        "\n",
        "    B = config[\"FAIRNESS_BATCH_SIZE\"]\n",
        "    N_GROUPS = config[\"NUM_RACE_GROUPS\"]\n",
        "    DEVICE = config[\"DEVICE\"]\n",
        "\n",
        "    if B % N_GROUPS != 0:\n",
        "        raise ValueError(\"FAIRNESS_BATCH_SIZE must be divisible by NUM_RACE_GROUPS\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z_fair = torch.randn(B, config[\"LATENT_DIM\"], device=DEVICE)\n",
        "        fake_images_fair = gen(z_fair, alpha, steps)\n",
        "        fake_images_512 = F.interpolate(fake_images_fair, size=(512, 512), mode='bilinear', align_corners=False)\n",
        "\n",
        "    diag_preds_logits = cdiag(fake_images_512).squeeze()\n",
        "    diag_preds_probs = torch.sigmoid(diag_preds_logits)\n",
        "\n",
        "    samples_per_group = B // N_GROUPS\n",
        "    tpr_per_group = []\n",
        "    for j in range(N_GROUPS):\n",
        "        start_idx = j * samples_per_group\n",
        "        end_idx = (j + 1) * samples_per_group\n",
        "        tpr = diag_preds_probs[start_idx:end_idx].mean()\n",
        "        tpr_per_group.append(tpr)\n",
        "\n",
        "    if len(tpr_per_group) > 1:\n",
        "        tpr_stack = torch.stack(tpr_per_group)\n",
        "\n",
        "        # 1. Fairness (Equality) Loss: Make them equal\n",
        "        loss_std = torch.std(tpr_stack)\n",
        "\n",
        "        # 2. Utility (Push) Loss: Make them HIGH\n",
        "        # We want the mean TPR to be 1.0. Loss is (1.0 - mean_tpr)\n",
        "        loss_push = 1.0 - torch.mean(tpr_stack)\n",
        "\n",
        "        # 3. Combine them using the push weight\n",
        "        loss_g_fair = loss_std + config[\"FAIRNESS_PUSH_WEIGHT\"] * loss_push\n",
        "\n",
        "    else:\n",
        "        loss_g_fair = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "    gen.train() # Set generator back to train mode\n",
        "    return loss_g_fair\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Training loop (MODIFIED)\n",
        "# ---------------------------\n",
        "def run_training():\n",
        "    DEVICE = CONFIG['DEVICE']\n",
        "    print(\"üöÄ Initializing Robust CAF-GAN Training...\")\n",
        "\n",
        "    # --- Models & Optimizers ---\n",
        "    gen = Generator(CONFIG['LATENT_DIM'], CONFIG['LATENT_DIM'], base_channels=512, img_channels=CONFIG['CHANNELS']).to(DEVICE)\n",
        "    disc = Discriminator(base_channels=512, img_channels=CONFIG['CHANNELS']).to(DEVICE)\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=CONFIG['G_LR'], betas=(CONFIG['B1'], CONFIG['B2']))\n",
        "    opt_disc = optim.Adam(disc.parameters(), lr=CONFIG['D_LR'], betas=(CONFIG['B1'], CONFIG['B2']))\n",
        "\n",
        "    # --- Load Frozen Critics (Cdiag and Cseg) ---\n",
        "    Cdiag, Cseg = load_critics(CONFIG['CDIAG_PATH'], CONFIG['CSEG_PATH'], DEVICE)\n",
        "\n",
        "    # --- Checkpoint Loading ---\n",
        "    checkpoint_path = os.path.join(CONFIG['OUTPUT_DIR'], \"caf_gan_checkpoint.pth\")\n",
        "    start_epoch, start_step, alpha = load_checkpoint(gen, disc, opt_gen, opt_disc, checkpoint_path, DEVICE)\n",
        "\n",
        "    train_df = pd.read_csv(CONFIG['TRAIN_CSV_PATH'])\n",
        "    fixed_noise = torch.randn(8, CONFIG['LATENT_DIM'], device=DEVICE)\n",
        "\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "\n",
        "    num_progressive_stages = len(CONFIG['PROGRESSIVE_EPOCHS'])\n",
        "\n",
        "    # --- Main Training Loop ---\n",
        "    for res_step in range(start_step, num_progressive_stages):\n",
        "        steps = res_step + 1\n",
        "        img_size = 4 * (2 ** steps)\n",
        "        if img_size > CONFIG['TARGET_IMG_SIZE']: break\n",
        "\n",
        "        loader_batch_size = CONFIG['BATCH_SIZES'][res_step]\n",
        "        num_epochs_for_res = CONFIG['PROGRESSIVE_EPOCHS'][res_step]\n",
        "\n",
        "        epoch_offset = start_epoch if res_step == start_step else 0\n",
        "        if epoch_offset > 0:\n",
        "            print(f\"Resuming at Epoch {epoch_offset+1}/{num_epochs_for_res} for this stage.\")\n",
        "        start_epoch = 0\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"üìà Starting Training for Resolution: {img_size}x{img_size}\")\n",
        "        print(f\"   Epochs for this stage: {num_epochs_for_res} | Batch Size: {loader_batch_size}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        transform = A.Compose([\n",
        "            A.Resize(width=img_size, height=img_size, interpolation=Image.LANCZOS),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        dataset = MIMIC_GAN_Dataset(train_df, CONFIG['IMAGE_DIR'], transform=transform)\n",
        "        loader = DataLoader(dataset, batch_size=loader_batch_size, shuffle=True, num_workers=CONFIG['NUM_WORKERS'], pin_memory=True, collate_fn=custom_collate)\n",
        "\n",
        "        for epoch in range(epoch_offset, num_epochs_for_res):\n",
        "            current_epoch_total = sum(CONFIG['PROGRESSIVE_EPOCHS'][:res_step]) + epoch + 1\n",
        "            print(f\"\\n--- Resolution {img_size}x{img_size} | Epoch {epoch+1}/{num_epochs_for_res} (Total: {current_epoch_total}) ---\")\n",
        "\n",
        "            # --- Alpha and Lambda Annealing ---\n",
        "            if num_epochs_for_res > 1:\n",
        "                alpha = min(1.0, (epoch + 1) / (num_epochs_for_res * 0.5))\n",
        "            else:\n",
        "                alpha = 1.0\n",
        "\n",
        "            clinical_fair_lambda_scale = 1.0\n",
        "            if epoch < CONFIG['LOSS_ANNEAL_START_EPOCH']:\n",
        "                clinical_fair_lambda_scale = 0.0\n",
        "            elif epoch < num_epochs_for_res:\n",
        "                clinical_fair_lambda_scale = max(0.0, (epoch - CONFIG['LOSS_ANNEAL_START_EPOCH']) / max(1, (num_epochs_for_res - CONFIG['LOSS_ANNEAL_START_EPOCH'])))\n",
        "\n",
        "            loop = tqdm(loader, leave=True)\n",
        "            for batch_idx, (real, labels, races) in enumerate(loop):\n",
        "                if real.nelement() == 0:\n",
        "                    continue\n",
        "                real = real.to(DEVICE)\n",
        "\n",
        "                # ========== Train Discriminator ==========\n",
        "                noise = torch.randn(real.shape[0], CONFIG['LATENT_DIM'], device=DEVICE)\n",
        "                fake = gen(noise, alpha, steps)\n",
        "\n",
        "                disc_real = disc(real, alpha, steps)\n",
        "                disc_fake = disc(fake.detach(), alpha, steps)\n",
        "\n",
        "                # R1 Gradient Penalty\n",
        "                gp = 0.0\n",
        "                if batch_idx % CONFIG['R1_REG_FREQ'] == 0:\n",
        "                    real.requires_grad = True\n",
        "                    disc_real_for_gp = disc(real, alpha, steps)\n",
        "                    grad = torch.autograd.grad(outputs=disc_real_for_gp.sum(), inputs=real, create_graph=True)[0]\n",
        "                    grad_penalty = (grad.view(grad.shape[0], -1).norm(2, dim=1) ** 2).mean()\n",
        "                    gp = CONFIG['LAMBDA_R1'] * grad_penalty\n",
        "                    real.requires_grad = False\n",
        "\n",
        "                loss_disc = (torch.mean(F.softplus(disc_fake)) + torch.mean(F.softplus(-disc_real))) + gp\n",
        "\n",
        "                opt_disc.zero_grad()\n",
        "                loss_disc.backward()\n",
        "                opt_disc.step()\n",
        "\n",
        "                # ========== Train Generator ==========\n",
        "                gen_fake = gen(noise, alpha, steps)\n",
        "                disc_fake_for_g = disc(gen_fake, alpha, steps)\n",
        "                loss_g_adv = torch.mean(F.softplus(-disc_fake_for_g))\n",
        "\n",
        "                loss_g_clinic = torch.tensor(0.0, device=DEVICE)\n",
        "                loss_g_fair = torch.tensor(0.0, device=DEVICE)\n",
        "                # --- üí° NEW UTILITY LOSS VARIABLE ---\n",
        "                loss_g_utility = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "                # --- Apply Robust Clinical, Fair, and Utility Losses ---\n",
        "                if clinical_fair_lambda_scale > 0.0 and img_size == CONFIG['TARGET_IMG_SIZE']:\n",
        "\n",
        "                    fake_512 = F.interpolate(gen_fake, size=(512, 512), mode='bilinear', align_corners=False)\n",
        "\n",
        "                    # --- 1. L_clinical (from Cseg) ---\n",
        "                    fake_masks = Cseg(fake_512)\n",
        "                    loss_g_clinic = calculate_clinical_loss(fake_masks, CONFIG)\n",
        "\n",
        "                    # --- 2. üí° NEW L_utility (from Cdiag) ---\n",
        "                    loss_g_utility = calculate_diag_utility_loss(fake_512, Cdiag)\n",
        "\n",
        "                    # --- 3. üí° UPGRADED L_fairness (from Cdiag) ---\n",
        "                    loss_g_fair = calculate_fairness_loss(gen, Cdiag, alpha, steps, CONFIG)\n",
        "\n",
        "\n",
        "                # --- üí° UPDATED Total Generator Loss ---\n",
        "                loss_gen = (\n",
        "                    loss_g_adv +\n",
        "                    clinical_fair_lambda_scale * CONFIG['LAMBDA_CLINIC'] * loss_g_clinic +\n",
        "                    clinical_fair_lambda_scale * CONFIG['LAMBDA_FAIR'] * loss_g_fair +\n",
        "                    clinical_fair_lambda_scale * CONFIG['LAMBDA_UTILITY'] * loss_g_utility\n",
        "                )\n",
        "\n",
        "                opt_gen.zero_grad()\n",
        "                loss_gen.backward()\n",
        "                opt_gen.step()\n",
        "\n",
        "                # --- üí° UPDATED TQDM Postfix ---\n",
        "                loop.set_postfix(\n",
        "                    D_loss=f\"{loss_disc.item():.4f}\",\n",
        "                    G_loss=f\"{loss_gen.item():.4f}\",\n",
        "                    G_adv=f\"{loss_g_adv.item():.4f}\",\n",
        "                    G_clinic=f\"{loss_g_clinic.item():.4f}\",\n",
        "                    G_fair=f\"{loss_g_fair.item():.4f}\",\n",
        "                    G_util=f\"{loss_g_utility.item():.4f}\",\n",
        "                    Alpha=f\"{alpha:.3f}\",\n",
        "                    Œª_Scale=f\"{clinical_fair_lambda_scale:.3f}\"\n",
        "                )\n",
        "\n",
        "                if batch_idx % CONFIG['CHECKPOINT_FREQ'] == 0:\n",
        "                    save_checkpoint(gen, disc, opt_gen, opt_disc, epoch, res_step, alpha, checkpoint_path)\n",
        "\n",
        "            # --- End of Epoch Actions ---\n",
        "            if (epoch + 1) % CONFIG['SAVE_IMG_FREQ'] == 0:\n",
        "                gen.eval()\n",
        "                with torch.no_grad():\n",
        "                    img_grid = gen(fixed_noise, alpha, steps).detach().cpu()\n",
        "                vutils.save_image(img_grid, os.path.join(CONFIG['IMAGE_OUTPUT_DIR'], f\"res_{img_size}_epoch_{current_epoch_total}.png\"), normalize=True, nrow=4)\n",
        "                print(f\"üñºÔ∏è Saved generated images for epoch {current_epoch_total}\")\n",
        "                gen.train()\n",
        "\n",
        "            save_checkpoint(gen, disc, opt_gen, opt_disc, epoch, res_step, alpha, checkpoint_path)\n",
        "            print(f\"üíæ End of epoch {current_epoch_total}. Checkpoint saved.\")\n",
        "\n",
        "    print(\"\\nüéâüéâüéâ CAF-GAN Training Complete! üéâüéâüéâ\")\n",
        "    torch.save(gen.state_dict(), os.path.join(CONFIG['OUTPUT_DIR'], \"caf_gan_generator_final.pth\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_training()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740,
          "referenced_widgets": [
            "3994a0b1ace54b158686e8ad2c71378d",
            "e1d98be8317f448eacd88bc6b19ddc63",
            "5024547030b24575a5e81748f8a930f1",
            "448e4065409c49b2931a6c5865587a7d",
            "560d36583e434f86bc2a1c00c1cffb4b",
            "9142d88d1a874d39becf5253cf54f40a",
            "b58490368d9e43f8806b9636f9347cdf",
            "ad5fd0e9e5554f37a463c41462737954",
            "1f53e32eb8cb4eadad36eef5166c641e",
            "cea8a479f274441aa94af6bd404fbf69",
            "db93872655644193b50147116c4dc64f",
            "e8df5618528f411fb1461e314b58e695",
            "382c23925b2e4fab867bcd4361eabd5a",
            "a569e2d2feb546ebafe30b7fdc2577b0",
            "ed657ae12b9c48a0ae9941ce88c7b37b",
            "3c3c2f521c2b4fb4921540e236649926",
            "ef1f6aeca11b41a5b7151a97a1120cb8",
            "b2341279d7af43aca4946fa75d952148",
            "980b90cb9ec24f889cd5e72a22d8547b",
            "1b0b1b47a59941daafa01590442804ef",
            "04f4071049254a73b51056a891efa3e4",
            "85914ad83ccf4940959e494255838e6e"
          ]
        },
        "id": "2aXdMkHRsjcE",
        "outputId": "57b8e19d-0648-4d89-fcd6-2af60494eb18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initializing Robust CAF-GAN Training...\n",
            "üß† Loading frozen critic models...\n",
            "   - Cdiag (Diagnostic Critic) loaded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3994a0b1ace54b158686e8ad2c71378d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8df5618528f411fb1461e314b58e695"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   - Cseg (Segmentation Critic) loaded.\n",
            "‚úÖ Resuming training from checkpoint: /content/drive/MyDrive/CAF-GAN/outputs/caf_gan_final/caf_gan_checkpoint.pth\n",
            "Resuming at Epoch 96/100 for this stage.\n",
            "\n",
            "==================================================\n",
            "üìà Starting Training for Resolution: 512x512\n",
            "   Epochs for this stage: 100 | Batch Size: 4\n",
            "==================================================\n",
            "\n",
            "--- Resolution 512x512 | Epoch 96/100 (Total: 276) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [09:24<00:00,  1.61s/it, Alpha=1.000, D_loss=0.0385, G_adv=1.5693, G_clinic=0.3712, G_fair=0.0170, G_loss=1.7518, G_util=0.0166, Œª_Scale=0.941]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è Saved generated images for epoch 276\n",
            "üíæ End of epoch 276. Checkpoint saved.\n",
            "\n",
            "--- Resolution 512x512 | Epoch 97/100 (Total: 277) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [09:03<00:00,  1.55s/it, Alpha=1.000, D_loss=0.2446, G_adv=1.9582, G_clinic=0.6295, G_fair=0.0939, G_loss=2.3463, G_util=0.2459, Œª_Scale=0.953]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è Saved generated images for epoch 277\n",
            "üíæ End of epoch 277. Checkpoint saved.\n",
            "\n",
            "--- Resolution 512x512 | Epoch 98/100 (Total: 278) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [09:03<00:00,  1.55s/it, Alpha=1.000, D_loss=0.2477, G_adv=4.4539, G_clinic=0.3505, G_fair=0.0439, G_loss=4.6482, G_util=0.0579, Œª_Scale=0.965]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è Saved generated images for epoch 278\n",
            "üíæ End of epoch 278. Checkpoint saved.\n",
            "\n",
            "--- Resolution 512x512 | Epoch 99/100 (Total: 279) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [09:04<00:00,  1.56s/it, Alpha=1.000, D_loss=0.1615, G_adv=5.8137, G_clinic=0.4206, G_fair=0.0370, G_loss=6.0320, G_util=0.0198, Œª_Scale=0.976]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è Saved generated images for epoch 279\n",
            "üíæ End of epoch 279. Checkpoint saved.\n",
            "\n",
            "--- Resolution 512x512 | Epoch 100/100 (Total: 280) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [09:06<00:00,  1.56s/it, Alpha=1.000, D_loss=0.5671, G_adv=4.5973, G_clinic=0.4247, G_fair=0.0915, G_loss=4.8767, G_util=0.1738, Œª_Scale=0.988]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è Saved generated images for epoch 280\n",
            "üíæ End of epoch 280. Checkpoint saved.\n",
            "\n",
            "üéâüéâüéâ CAF-GAN Training Complete! üéâüéâüéâ\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3994a0b1ace54b158686e8ad2c71378d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1d98be8317f448eacd88bc6b19ddc63",
              "IPY_MODEL_5024547030b24575a5e81748f8a930f1",
              "IPY_MODEL_448e4065409c49b2931a6c5865587a7d"
            ],
            "layout": "IPY_MODEL_560d36583e434f86bc2a1c00c1cffb4b"
          }
        },
        "e1d98be8317f448eacd88bc6b19ddc63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9142d88d1a874d39becf5253cf54f40a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b58490368d9e43f8806b9636f9347cdf",
            "value": "config.json:‚Äá100%"
          }
        },
        "5024547030b24575a5e81748f8a930f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad5fd0e9e5554f37a463c41462737954",
            "max": 156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f53e32eb8cb4eadad36eef5166c641e",
            "value": 156
          }
        },
        "448e4065409c49b2931a6c5865587a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea8a479f274441aa94af6bd404fbf69",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_db93872655644193b50147116c4dc64f",
            "value": "‚Äá156/156‚Äá[00:00&lt;00:00,‚Äá12.9kB/s]"
          }
        },
        "560d36583e434f86bc2a1c00c1cffb4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9142d88d1a874d39becf5253cf54f40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b58490368d9e43f8806b9636f9347cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad5fd0e9e5554f37a463c41462737954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f53e32eb8cb4eadad36eef5166c641e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cea8a479f274441aa94af6bd404fbf69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db93872655644193b50147116c4dc64f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8df5618528f411fb1461e314b58e695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_382c23925b2e4fab867bcd4361eabd5a",
              "IPY_MODEL_a569e2d2feb546ebafe30b7fdc2577b0",
              "IPY_MODEL_ed657ae12b9c48a0ae9941ce88c7b37b"
            ],
            "layout": "IPY_MODEL_3c3c2f521c2b4fb4921540e236649926"
          }
        },
        "382c23925b2e4fab867bcd4361eabd5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1f6aeca11b41a5b7151a97a1120cb8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b2341279d7af43aca4946fa75d952148",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "a569e2d2feb546ebafe30b7fdc2577b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_980b90cb9ec24f889cd5e72a22d8547b",
            "max": 87275112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b0b1b47a59941daafa01590442804ef",
            "value": 87275112
          }
        },
        "ed657ae12b9c48a0ae9941ce88c7b37b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04f4071049254a73b51056a891efa3e4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_85914ad83ccf4940959e494255838e6e",
            "value": "‚Äá87.3M/87.3M‚Äá[00:01&lt;00:00,‚Äá99.8MB/s]"
          }
        },
        "3c3c2f521c2b4fb4921540e236649926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1f6aeca11b41a5b7151a97a1120cb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2341279d7af43aca4946fa75d952148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "980b90cb9ec24f889cd5e72a22d8547b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0b1b47a59941daafa01590442804ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04f4071049254a73b51056a891efa3e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85914ad83ccf4940959e494255838e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}