{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrhYk2bxB_Yx",
        "outputId": "1c38db05-3849-4409-ee2c-6b53cf0315e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaXxPXf_CZ0A",
        "outputId": "9b23d711-d0a6-4c99-c2fb-cda219d74e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CAF-GAN\n",
            "‚úÖ Workspace ready and images unzipped.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Navigate to your project directory in Google Drive\n",
        "%cd /content/drive/MyDrive/CAF-GAN/\n",
        "\n",
        "# Unzip the image files (this might take a few minutes)\n",
        "# The -q makes the output quiet, -n prevents unzipping if already done\n",
        "!unzip -q -n mimic-cxr-jpg-2.0.0.zip\n",
        "\n",
        "print(\"‚úÖ Workspace ready and images unzipped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d8VZ0P_1C4Kq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a58913-0186-49d9-a830-0240856fdc00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pyyaml pandas scikit-learn albumentations segmentation-models-pytorch -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    CAF-GAN Generator Network (DCGAN architecture).\n",
        "    Takes a latent vector z and outputs a 256x256 grayscale image.\n",
        "    Output is normalized between -1 and 1 using Tanh.\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim, channels=1):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: latent_dim x 1 x 1\n",
        "            nn.ConvTranspose2d(latent_dim, 1024, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(True),\n",
        "            # State: 1024 x 4 x 4\n",
        "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            # State: 512 x 8 x 8\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # State: 256 x 16 x 16\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # State: 128 x 32 x 32\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            # State: 64 x 64 x 64\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            # State: 32 x 128 x 128\n",
        "            nn.ConvTranspose2d(32, channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # Output: channels x 256 x 256\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "LBdc2V1RzmAX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rDVWP3_8C-gw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# This single dataset file will serve both critic training scripts.\n",
        "\n",
        "class MIMICCXRClassifierDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for the Cdiag (classification) task.\n",
        "    - Loads a JPG image.\n",
        "    - Converts it to RGB (as required by ResNet).\n",
        "    - Returns the image and its corresponding Pneumonia label.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, image_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        subject_id = str(row['subject_id'])\n",
        "        study_id = str(row['study_id'])\n",
        "        dicom_id = row['dicom_id']\n",
        "\n",
        "        # Construct the path to the JPG image\n",
        "        image_path = os.path.join(\n",
        "            self.image_dir, f'p{subject_id[:2]}', f'p{subject_id}', f's{study_id}', f'{dicom_id}.jpg'\n",
        "        )\n",
        "\n",
        "        # Load image and convert to a numpy array in RGB format\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        # Get the label\n",
        "        label = torch.tensor(row['Pneumonia'], dtype=torch.float32)\n",
        "\n",
        "        return image, label.unsqueeze(0)\n",
        "\n",
        "\n",
        "class MIMICXRSegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for the Cseg (segmentation) task.\n",
        "    - Loads a JPG image (as 3-channel RGB). <--- UPDATED\n",
        "    - Loads its corresponding pre-generated PNG mask.\n",
        "    - Returns both the image and the mask.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, image_dir, mask_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        subject_id = str(row['subject_id'])\n",
        "        study_id = str(row['study_id'])\n",
        "        dicom_id = row['dicom_id']\n",
        "\n",
        "        image_path = os.path.join(\n",
        "            self.image_dir, f'p{subject_id[:2]}', f'p{subject_id}', f's{study_id}', f'{dicom_id}.jpg'\n",
        "        )\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{dicom_id}.png\")\n",
        "\n",
        "        # --- KEY CHANGE ---\n",
        "        # Load image and convert to RGB to match the model's expected input channels.\n",
        "        image = np.array(Image.open(image_path).convert(\"RGB\"), dtype=np.float32)\n",
        "\n",
        "        # Load mask as grayscale\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "\n",
        "        # RESIZE THE IMAGE to match the mask size (256√ó256) BEFORE augmentation\n",
        "        image = np.array(Image.fromarray(image.astype(np.uint8)).resize((256, 256), Image.BILINEAR))\n",
        "\n",
        "        # Normalize mask values from [0, 255] to [0.0, 1.0]\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        # Apply augmentations (Albumentations will now see matching input sizes from the Resize transform)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        # Add a channel dimension for the mask for consistency\n",
        "        return image, mask.unsqueeze(0)\n",
        "\n",
        "class MIMICCXR_GANDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for the main GAN training.\n",
        "    - Loads a JPG image (as grayscale).\n",
        "    - Returns the image, its Pneumonia label, and the one-hot encoded race group.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, image_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Pre-process sensitive attributes\n",
        "        self.df['race_group'] = self.df['race_group'].astype('category')\n",
        "        self.race_categories = self.df['race_group'].cat.categories\n",
        "        self.one_hot_races = pd.get_dummies(self.df['race_group'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        subject_id = str(row['subject_id'])\n",
        "        study_id = str(row['study_id'])\n",
        "        dicom_id = row['dicom_id']\n",
        "\n",
        "        image_path = os.path.join(\n",
        "            self.image_dir, f'p{subject_id[:2]}', f'p{subject_id}', f's{study_id}', f'{dicom_id}.jpg'\n",
        "        )\n",
        "\n",
        "        # Load the image and convert to \"RGB\"\n",
        "        image = Image.open(image_path).convert(\"RGB\") # <-- CHANGE \"L\" to \"RGB\"\n",
        "        image = np.array(image, dtype=np.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        label = torch.tensor(row['Pneumonia'], dtype=torch.float32)\n",
        "        race = torch.tensor(self.one_hot_races.iloc[idx].values, dtype=torch.float32)\n",
        "\n",
        "        return image, label, race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "87Yg_DB4DIR3"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  # Configuration for the final evaluation (Phase D)\n",
        "\n",
        "  # --- Paths ---\n",
        "  # Path to the trained generator from Phase C\n",
        "  \"GENERATOR_CHECKPOINT\": \"/content/drive/MyDrive/CAF-GAN/outputs/gan/netG_epoch_200.pth\", #<-- IMPORTANT: Update this to your best generator checkpoint\n",
        "  \"REAL_DATA_CSV_TEST\": \"/content/drive/MyDrive/CAF-GAN/data/splits/test.csv\",\n",
        "  \"IMAGE_DIR_REAL\": \"/content/drive/MyDrive/CAF-GAN/mimic-cxr-jpg-2.0.0/files/\",\n",
        "\n",
        "  # --- Synthetic Data Generation ---\n",
        "  \"SYNTHETIC_DATA_DIR\": \"/content/drive/MyDrive/CAF-GAN/data/synthetic_images/\",\n",
        "  \"SYNTHETIC_CSV_PATH\": \"/content/drive/MyDrive/CAF-GAN/data/synthetic_images/labels.csv\",\n",
        "  \"NUM_SYNTHETIC_IMAGES\": 2000, # Generate a dataset of the same size as our original subset\n",
        "  \"GENERATION_BATCH_SIZE\": 32,\n",
        "\n",
        "  # --- Downstream Classifier Training ---\n",
        "  \"CLASSIFIER_OUTPUT_DIR\": \"outputs/downstream_classifier/\",\n",
        "  \"CLASSIFIER_MODEL_NAME\": \"best_synth_trained_classifier.pth\",\n",
        "  \"IMG_SIZE\": 256,\n",
        "  \"BATCH_SIZE\": 32,\n",
        "  \"EPOCHS\": 15,\n",
        "  \"LEARNING_RATE\": 0.0001,\n",
        "\n",
        "  # --- System ---\n",
        "  \"DEVICE\": \"cuda\",\n",
        "  \"NUM_WORKERS\": 2,\n",
        "  \"LATENT_DIM\": 128,\n",
        "  \"CHANNELS\": 3\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LRfvu61DWoV",
        "outputId": "9b275edc-e62f-4fac-b235-bfcb32333353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---  b∆∞·ªõc 1: Generating Synthetic Dataset ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:21<00:00,  2.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated 2000 synthetic images and saved labels to /content/drive/MyDrive/CAF-GAN/data/synthetic_images/labels.csv\n",
            "\n",
            "--- b∆∞·ªõc 2: Training Downstream Classifier on Synthetic Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:24<00:00,  2.55it/s]\n",
            "Epoch 2/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:24<00:00,  2.62it/s]\n",
            "Epoch 3/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.69it/s]\n",
            "Epoch 4/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.66it/s]\n",
            "Epoch 5/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.65it/s]\n",
            "Epoch 6/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.65it/s]\n",
            "Epoch 7/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.67it/s]\n",
            "Epoch 8/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.66it/s]\n",
            "Epoch 9/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.66it/s]\n",
            "Epoch 10/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.64it/s]\n",
            "Epoch 11/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.67it/s]\n",
            "Epoch 12/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.67it/s]\n",
            "Epoch 13/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.67it/s]\n",
            "Epoch 14/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.64it/s]\n",
            "Epoch 15/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:23<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Downstream classifier training complete.\n",
            "\n",
            "--- b∆∞·ªõc 3: Evaluating on Real Test Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:46<00:00, 22.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- üìä Evaluation Report ---\n",
            "Trained on 2000 synthetic images. Evaluated on 301 real test images.\n",
            "\n",
            "## üéØ Overall Performance (Utility)\n",
            "**AUC:** 0.4849\n",
            "**Accuracy:** 0.5548\n",
            "**F1-Score:** 0.3232\n",
            "\n",
            "## ‚öñÔ∏è Fairness Performance\n",
            "True Positive Rate (TPR) by Group:\n",
            "  - WHITE: 0.2911\n",
            "  - OTHER: 0.1250\n",
            "  - BLACK: 0.2778\n",
            "  - HISPANIC/LATINO: 0.3333\n",
            "  - ASIAN: 0.2500\n",
            "**Equal Opportunity Difference (Max TPR - Min TPR): 0.2083**\n",
            "\n",
            "--- Evaluation Complete ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as vutils\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# We need to import our project's models and datasets\n",
        "# from src.models.generator import Generator\n",
        "# from src.data.dataset import MIMICCXRClassifierDataset\n",
        "\n",
        "def generate_synthetic_data(config, device):\n",
        "    \"\"\"Loads a trained generator and creates a new synthetic dataset.\"\"\"\n",
        "    print(\"---  b∆∞·ªõc 1: Generating Synthetic Dataset ---\")\n",
        "    os.makedirs(config['SYNTHETIC_DATA_DIR'], exist_ok=True)\n",
        "\n",
        "    # Load Generator\n",
        "    netG = Generator(config['LATENT_DIM'], config['CHANNELS']).to(device)\n",
        "    netG.load_state_dict(torch.load(config['GENERATOR_CHECKPOINT'], map_location=device))\n",
        "    netG.eval()\n",
        "\n",
        "    # Create balanced labels for the synthetic data\n",
        "    num_positive = config['NUM_SYNTHETIC_IMAGES'] // 2\n",
        "    labels = np.array([1] * num_positive + [0] * (config['NUM_SYNTHETIC_IMAGES'] - num_positive))\n",
        "    np.random.shuffle(labels)\n",
        "\n",
        "    image_ids = []\n",
        "    generated_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, config['NUM_SYNTHETIC_IMAGES'], config['GENERATION_BATCH_SIZE']), desc=\"Generating Images\"):\n",
        "            batch_size = min(config['GENERATION_BATCH_SIZE'], config['NUM_SYNTHETIC_IMAGES'] - i)\n",
        "            noise = torch.randn(batch_size, config['LATENT_DIM'], 1, 1, device=device)\n",
        "            fake_imgs = netG(noise)\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                img_idx = i + j\n",
        "                image_id = f\"synth_{img_idx:05d}.jpg\"\n",
        "                vutils.save_image(fake_imgs[j], os.path.join(config['SYNTHETIC_DATA_DIR'], image_id), normalize=True)\n",
        "                image_ids.append(image_id)\n",
        "                generated_labels.append(labels[img_idx])\n",
        "\n",
        "    # Save the labels to a CSV file\n",
        "    synthetic_df = pd.DataFrame({'image_id': image_ids, 'Pneumonia': generated_labels})\n",
        "    synthetic_df.to_csv(config['SYNTHETIC_CSV_PATH'], index=False)\n",
        "    print(f\"‚úÖ Generated {config['NUM_SYNTHETIC_IMAGES']} synthetic images and saved labels to {config['SYNTHETIC_CSV_PATH']}\")\n",
        "    return synthetic_df\n",
        "\n",
        "def train_downstream_classifier(config, device):\n",
        "    \"\"\"Trains a new classifier from scratch on the synthetic dataset.\"\"\"\n",
        "    print(\"\\n--- b∆∞·ªõc 2: Training Downstream Classifier on Synthetic Data ---\")\n",
        "    os.makedirs(config['CLASSIFIER_OUTPUT_DIR'], exist_ok=True)\n",
        "\n",
        "    transform = A.Compose([\n",
        "        A.Resize(config['IMG_SIZE'], config['IMG_SIZE']),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    # We need a small, modified Dataset class for the synthetic data\n",
        "    class SyntheticDataset(Dataset):\n",
        "        def __init__(self, df, image_dir, transform):\n",
        "            self.df = df\n",
        "            self.image_dir = image_dir\n",
        "            self.transform = transform\n",
        "        def __len__(self):\n",
        "            return len(self.df)\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.df.iloc[idx]\n",
        "            image_path = os.path.join(self.image_dir, row['image_id'])\n",
        "            image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "            label = torch.tensor(row['Pneumonia'], dtype=torch.float32)\n",
        "            if self.transform:\n",
        "                image = self.transform(image=image)['image']\n",
        "            return image, label.unsqueeze(0)\n",
        "\n",
        "    synth_df = pd.read_csv(config['SYNTHETIC_CSV_PATH'])\n",
        "    train_dataset = SyntheticDataset(synth_df, config['SYNTHETIC_DATA_DIR'], transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['BATCH_SIZE'], shuffle=True, num_workers=config['NUM_WORKERS'])\n",
        "\n",
        "    model = models.resnet50(weights='IMAGENET1K_V1')\n",
        "    model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])\n",
        "\n",
        "    for epoch in range(config['EPOCHS']):\n",
        "        model.train()\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['EPOCHS']}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(config['CLASSIFIER_OUTPUT_DIR'], config['CLASSIFIER_MODEL_NAME']))\n",
        "    print(\"‚úÖ Downstream classifier training complete.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_on_real_data(classifier, config, device):\n",
        "    \"\"\"Evaluates the synthetically-trained classifier on the real test set.\"\"\"\n",
        "    print(\"\\n--- b∆∞·ªõc 3: Evaluating on Real Test Data ---\")\n",
        "\n",
        "    transform = A.Compose([\n",
        "        A.Resize(config['IMG_SIZE'], config['IMG_SIZE']),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    test_df = pd.read_csv(config['REAL_DATA_CSV_TEST'])\n",
        "    # The original dataset needs a different class that knows the MIMIC path structure\n",
        "    class RealTestDataset(MIMICCXRClassifierDataset):\n",
        "        def __getitem__(self, idx):\n",
        "            image, label = super().__getitem__(idx)\n",
        "            race = self.df.iloc[idx]['race_group']\n",
        "            return image, label, race\n",
        "\n",
        "    test_dataset = RealTestDataset(test_df, config['IMAGE_DIR_REAL'], transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=config['NUM_WORKERS'])\n",
        "\n",
        "    classifier.eval()\n",
        "    all_preds, all_labels, all_races = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, races in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = classifier(images)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy().flatten())\n",
        "            all_races.extend(races)\n",
        "\n",
        "    # --- Calculate Metrics ---\n",
        "    df_results = pd.DataFrame({'label': all_labels, 'pred_prob': all_preds, 'race': all_races})\n",
        "    df_results['prediction'] = (df_results['pred_prob'] > 0.5).astype(int)\n",
        "\n",
        "    # Utility Metrics\n",
        "    auc = roc_auc_score(df_results['label'], df_results['pred_prob'])\n",
        "    accuracy = accuracy_score(df_results['label'], df_results['prediction'])\n",
        "    f1 = f1_score(df_results['label'], df_results['prediction'])\n",
        "\n",
        "    # Fairness Metrics (Equal Opportunity Difference)\n",
        "    tpr_per_group = {}\n",
        "    for group in df_results['race'].unique():\n",
        "        group_df = df_results[df_results['race'] == group]\n",
        "        tn, fp, fn, tp = confusion_matrix(group_df['label'], group_df['prediction'], labels=[0,1]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        tpr_per_group[group] = tpr\n",
        "\n",
        "    eod = max(tpr_per_group.values()) - min(tpr_per_group.values())\n",
        "\n",
        "    # --- Print Report ---\n",
        "    print(\"\\n--- üìä Evaluation Report ---\")\n",
        "    print(f\"Trained on {config['NUM_SYNTHETIC_IMAGES']} synthetic images. Evaluated on {len(df_results)} real test images.\")\n",
        "    print(\"\\n## üéØ Overall Performance (Utility)\")\n",
        "    print(f\"**AUC:** {auc:.4f}\")\n",
        "    print(f\"**Accuracy:** {accuracy:.4f}\")\n",
        "    print(f\"**F1-Score:** {f1:.4f}\")\n",
        "\n",
        "    print(\"\\n## ‚öñÔ∏è Fairness Performance\")\n",
        "    print(\"True Positive Rate (TPR) by Group:\")\n",
        "    for group, tpr in tpr_per_group.items():\n",
        "        print(f\"  - {group}: {tpr:.4f}\")\n",
        "    print(f\"**Equal Opportunity Difference (Max TPR - Min TPR): {eod:.4f}**\")\n",
        "    print(\"\\n--- Evaluation Complete ---\")\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    \"\"\"Main function to run the entire evaluation pipeline.\"\"\"\n",
        "    # with open('configs/evaluate.yaml', 'r') as f:\n",
        "    #     config = yaml.safe_load(f)\n",
        "\n",
        "    device = config['DEVICE'] if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # STEP 1\n",
        "    generate_synthetic_data(config, device)\n",
        "\n",
        "    # STEP 2\n",
        "    trained_classifier = train_downstream_classifier(config, device)\n",
        "\n",
        "    # STEP 3\n",
        "    evaluate_on_real_data(trained_classifier, config, device)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(config)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}