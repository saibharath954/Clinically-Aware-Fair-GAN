{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml pandas scikit-learn albumentations segmentation-models-pytorch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing Robust CAF-GAN Training...\n",
      "ğŸ§  Loading frozen critic models...\n",
      "   - Cdiag (Diagnostic Critic) loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3994a0b1ace54b158686e8ad2c71378d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8df5618528f411fb1461e314b58e695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Cseg (Segmentation Critic) loaded.\n",
      "âœ… Resuming training from checkpoint: /content/drive/MyDrive/CAF-GAN/outputs/caf_gan_final/caf_gan_checkpoint.pth\n",
      "Resuming at Epoch 96/100 for this stage.\n",
      "\n",
      "==================================================\n",
      "ğŸ“ˆ Starting Training for Resolution: 512x512\n",
      "   Epochs for this stage: 100 | Batch Size: 4\n",
      "==================================================\n",
      "\n",
      "--- Resolution 512x512 | Epoch 96/100 (Total: 276) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [09:24<00:00,  1.61s/it, Alpha=1.000, D_loss=0.0385, G_adv=1.5693, G_clinic=0.3712, G_fair=0.0170, G_loss=1.7518, G_util=0.0166, Î»_Scale=0.941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ Saved generated images for epoch 276\n",
      "ğŸ’¾ End of epoch 276. Checkpoint saved.\n",
      "\n",
      "--- Resolution 512x512 | Epoch 97/100 (Total: 277) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [09:03<00:00,  1.55s/it, Alpha=1.000, D_loss=0.2446, G_adv=1.9582, G_clinic=0.6295, G_fair=0.0939, G_loss=2.3463, G_util=0.2459, Î»_Scale=0.953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ Saved generated images for epoch 277\n",
      "ğŸ’¾ End of epoch 277. Checkpoint saved.\n",
      "\n",
      "--- Resolution 512x512 | Epoch 98/100 (Total: 278) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [09:03<00:00,  1.55s/it, Alpha=1.000, D_loss=0.2477, G_adv=4.4539, G_clinic=0.3505, G_fair=0.0439, G_loss=4.6482, G_util=0.0579, Î»_Scale=0.965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ Saved generated images for epoch 278\n",
      "ğŸ’¾ End of epoch 278. Checkpoint saved.\n",
      "\n",
      "--- Resolution 512x512 | Epoch 99/100 (Total: 279) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [09:04<00:00,  1.56s/it, Alpha=1.000, D_loss=0.1615, G_adv=5.8137, G_clinic=0.4206, G_fair=0.0370, G_loss=6.0320, G_util=0.0198, Î»_Scale=0.976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ Saved generated images for epoch 279\n",
      "ğŸ’¾ End of epoch 279. Checkpoint saved.\n",
      "\n",
      "--- Resolution 512x512 | Epoch 100/100 (Total: 280) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [09:06<00:00,  1.56s/it, Alpha=1.000, D_loss=0.5671, G_adv=4.5973, G_clinic=0.4247, G_fair=0.0915, G_loss=4.8767, G_util=0.1738, Î»_Scale=0.988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ Saved generated images for epoch 280\n",
      "ğŸ’¾ End of epoch 280. Checkpoint saved.\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ CAF-GAN Training Complete! ğŸ‰ğŸ‰ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "CONFIG = {\n",
    "    \"IMAGE_DIR\": \"/content/drive/MyDrive/CAF-GAN/mimic-cxr-jpg-2.0.0/files/\",\n",
    "    \"TRAIN_CSV_PATH\": \"/content/drive/MyDrive/CAF-GAN/data/splits/train.csv\",\n",
    "    \"CDIAG_PATH\": \"/content/drive/MyDrive/CAF-GAN/outputs/cdiag_512/best_cdiag_512.pth\",\n",
    "    \"CSEG_PATH\": \"/content/drive/MyDrive/CAF-GAN/outputs/cseg_512/best_cseg_512.pth\",\n",
    "    \"OUTPUT_DIR\": \"/content/drive/MyDrive/CAF-GAN/outputs/caf_gan_final/\",\n",
    "    \"IMAGE_OUTPUT_DIR\": \"/content/drive/MyDrive/CAF-GAN/outputs/caf_gan_final/generated_images/\",\n",
    "    \"TARGET_IMG_SIZE\": 512,\n",
    "    \"LATENT_DIM\": 512,\n",
    "    \"CHANNELS\": 3,\n",
    "    \"PROGRESSIVE_EPOCHS\": [20, 20, 20, 40, 40, 40, 100],  # 7 entries for 8..512\n",
    "    \"BATCH_SIZES\": [8, 8, 8, 8, 8, 8, 4],\n",
    "    \"G_LR\": 1e-3,\n",
    "    \"D_LR\": 1e-3,\n",
    "    \"B1\": 0.0,\n",
    "    \"B2\": 0.99,\n",
    "    \"R1_REG_FREQ\": 16,\n",
    "    \"LAMBDA_R1\": 10,\n",
    "    \"LAMBDA_CLINIC\": 0.5,  ### ROBUSTNESS: Tuned weight\n",
    "    \"LAMBDA_FAIR\": 0.2,   ### ROBUSTNESS: Tuned weight\n",
    "\n",
    "    # --- ğŸ’¡ NEW UTILITY LOSS (FROM EVALUATION FEEDBACK) ---\\n\",\n",
    "    \"LAMBDA_UTILITY\": 0.3, # Pushes generator to make diagnosable Pneumonia\"\n",
    "\n",
    "    \"LOSS_ANNEAL_START_EPOCH\": 15,\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"NUM_WORKERS\": 2,\n",
    "    \"LOG_FREQ\": 100,\n",
    "    \"SAVE_IMG_FREQ\": 1,\n",
    "    \"CHECKPOINT_FREQ\": 500,\n",
    "\n",
    "    ### ROBUSTNESS FIX: Parameters for new L_clinical and L_fairness\n",
    "    \"CLINICAL_LOSS_TYPE\": \"area\", # 'area' implements L_area\n",
    "    \"PLAUSIBLE_LUNG_AREA_MEAN\": 0.220646, # Mean area of lungs as % of image (NEEDS TO BE PRE-CALCULATED from your Cseg on real data)\n",
    "    \"PLAUSIBLE_LUNG_AREA_STD\": 0.066277, # Std dev of lung area (NEEDS TO BE PRE-CALCULATED)\n",
    "    \"FAIRNESS_BATCH_SIZE\": 30, # Large, stable batch size for fairness loss\n",
    "    \"NUM_RACE_GROUPS\": 5, # From your dataset (White, Black, Asian, Hispanic, Other)\n",
    "\n",
    "    # --- ğŸ’¡ NEW FAIRNESS PUSH (FROM EVALUATION FEEDBACK) ---\",\n",
    "    \"FAIRNESS_PUSH_WEIGHT\": 0.5 # Balances std(TPR) vs. (1 - mean(TPR))\\n\",\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['IMAGE_OUTPUT_DIR'], exist_ok=True)\n",
    "os.makedirs(CONFIG['OUTPUT_DIR'], exist_ok=True)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Basic building blocks\n",
    "# (PixelNorm, WSConv2d, InjectNoise, AdaIN)\n",
    "# ---------------------------\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-8\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x * self.scale)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "        return out\n",
    "\n",
    "class InjectNoise(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device=x.device)\n",
    "        return x + self.weight * noise\n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.style_scale = nn.Linear(w_dim, channels)\n",
    "        self.style_bias = nn.Linear(w_dim, channels)\n",
    "    def forward(self, x, w):\n",
    "        x = self.instance_norm(x)\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style_scale * x + style_bias\n",
    "\n",
    "# ---------------------------\n",
    "# Mapping & Generator blocks\n",
    "# (MappingNetwork, GenBlock, Generator)\n",
    "# ---------------------------\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "        super().__init__()\n",
    "        layers = [PixelNorm()]\n",
    "        for i in range(8):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(z_dim, w_dim))\n",
    "            else:\n",
    "                layers.append(nn.Linear(w_dim, w_dim))\n",
    "            if i < 7:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.mapping(x)\n",
    "\n",
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.inject_noise1 = InjectNoise(out_channels)\n",
    "        self.inject_noise2 = InjectNoise(out_channels)\n",
    "        self.adain1 = AdaIN(out_channels, w_dim)\n",
    "        self.adain2 = AdaIN(out_channels, w_dim)\n",
    "    def forward(self, x, w):\n",
    "        x = self.leaky(self.inject_noise1(self.conv1(x)))\n",
    "        x = self.adain1(x, w)\n",
    "        x = self.leaky(self.inject_noise2(self.conv2(x)))\n",
    "        x = self.adain2(x, w)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim, base_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.starting_const = nn.Parameter(torch.randn(1, base_channels, 4, 4))\n",
    "        self.map = MappingNetwork(z_dim, w_dim)\n",
    "        self.initial_conv = WSConv2d(base_channels, base_channels, kernel_size=3, padding=1)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.factors = [512, 512, 512, 256, 128, 64, 32, 16]\n",
    "        assert base_channels == self.factors[0]\n",
    "        self.prog_blocks = nn.ModuleList()\n",
    "        self.to_rgbs = nn.ModuleList()\n",
    "        self.to_rgbs.append(WSConv2d(self.factors[0], img_channels, kernel_size=1, padding=0))\n",
    "        for i in range(1, len(self.factors)):\n",
    "            in_c = self.factors[i-1]\n",
    "            out_c = self.factors[i]\n",
    "            self.prog_blocks.append(GenBlock(in_c, out_c, w_dim))\n",
    "            self.to_rgbs.append(WSConv2d(out_c, img_channels, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, z, alpha, steps):\n",
    "        w = self.map(z)\n",
    "        batch = z.shape[0]\n",
    "        x = self.starting_const.repeat(batch, 1, 1, 1)\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.leaky(x)\n",
    "        if steps == 0:\n",
    "            out = self.to_rgbs[0](x)\n",
    "            return torch.tanh(out)\n",
    "        prev = None\n",
    "        for step in range(1, steps + 1):\n",
    "            prev = x\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            x = self.prog_blocks[step - 1](x, w)\n",
    "        final_out = self.to_rgbs[steps](x)\n",
    "        if alpha < 1.0 and prev is not None:\n",
    "            prev_rgb = self.to_rgbs[steps - 1](prev)\n",
    "            prev_rgb_upsampled = F.interpolate(prev_rgb, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            out = alpha * final_out + (1.0 - alpha) * prev_rgb_upsampled\n",
    "        else:\n",
    "            out = final_out\n",
    "        return torch.tanh(out)\n",
    "\n",
    "# ---------------------------\n",
    "# Discriminator\n",
    "# (DiscBlock, Discriminator)\n",
    "# ---------------------------\n",
    "class DiscBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, in_channels)\n",
    "        self.conv2 = WSConv2d(in_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, base_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.factors = [512, 512, 512, 256, 128, 64, 32, 16]\n",
    "        assert base_channels == self.factors[0]\n",
    "        self.from_rgbs = nn.ModuleList([WSConv2d(img_channels, ch, kernel_size=1, padding=0) for ch in self.factors])\n",
    "        self.prog_blocks = nn.ModuleList()\n",
    "        for i in range(1, len(self.factors)):\n",
    "            in_c = self.factors[i]\n",
    "            out_c = self.factors[i-1]\n",
    "            self.prog_blocks.append(DiscBlock(in_c, out_c))\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(self.factors[0] + 1, self.factors[0]), self.leaky,\n",
    "            WSConv2d(self.factors[0], self.factors[0], kernel_size=4, padding=0, stride=1), self.leaky,\n",
    "            WSConv2d(self.factors[0], 1, kernel_size=1, padding=0, stride=1)\n",
    "        )\n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.leaky(self.from_rgbs[steps](x))\n",
    "        for s in range(steps, 0, -1):\n",
    "            out = self.prog_blocks[s - 1](out)\n",
    "            out = F.avg_pool2d(out, kernel_size=2)\n",
    "            if alpha < 1.0 and s == steps:\n",
    "                downscaled_image = F.avg_pool2d(x, kernel_size=2)\n",
    "                low_res_features = self.leaky(self.from_rgbs[s - 1](downscaled_image))\n",
    "                out = alpha * out + (1.0 - alpha) * low_res_features\n",
    "        std = torch.std(out, dim=0, unbiased=False).mean().view(1, 1, 1, 1)\n",
    "        std_map = std.repeat(out.shape[0], 1, out.shape[2], out.shape[3])\n",
    "        out = torch.cat([out, std_map], dim=1)\n",
    "        out = self.final_block(out)\n",
    "        return out.view(out.shape[0], -1)\n",
    "\n",
    "# --------------------\n",
    "# Dataset & utilities\n",
    "# --------------------\n",
    "class MIMIC_GAN_Dataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.df['race_group'] = self.df['race_group'].astype('category')\n",
    "        self.one_hot_races = pd.get_dummies(self.df['race_group']).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        subject_id = str(row['subject_id'])\n",
    "        study_id = str(row['study_id'])\n",
    "        dicom_id = row['dicom_id']\n",
    "        image_path = os.path.join(self.image_dir, f'p{subject_id[:2]}', f'p{subject_id}', f's{study_id}', f'{dicom_id}.jpg')\n",
    "        try:\n",
    "            image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        except Exception:\n",
    "            return None, None, None\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        label = torch.tensor(row['Pneumonia'], dtype=torch.float32)\n",
    "        race = torch.tensor(self.one_hot_races.iloc[idx].values, dtype=torch.float32)\n",
    "        return image, label, race\n",
    "\n",
    "def custom_collate(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    if not batch:\n",
    "        return torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "def load_critics(cdiag_path, cseg_path, device):\n",
    "    print(\"ğŸ§  Loading frozen critic models...\")\n",
    "    Cdiag = models.resnet50()\n",
    "    Cdiag.fc = nn.Linear(Cdiag.fc.in_features, 1)\n",
    "    Cdiag.load_state_dict(torch.load(cdiag_path, map_location=device))\n",
    "    Cdiag.eval().to(device)\n",
    "    for param in Cdiag.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"   - Cdiag (Diagnostic Critic) loaded.\")\n",
    "\n",
    "    Cseg = smp.Unet(\"resnet34\", in_channels=3, classes=1)\n",
    "    Cseg.load_state_dict(torch.load(cseg_path, map_location=device))\n",
    "    Cseg.eval().to(device)\n",
    "    for param in Cseg.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"   - Cseg (Segmentation Critic) loaded.\")\n",
    "    return Cdiag, Cseg\n",
    "\n",
    "def save_checkpoint(gen, disc, opt_gen, opt_disc, epoch, step, alpha, path):\n",
    "    state = {\n",
    "        'gen': gen.state_dict(), 'disc': disc.state_dict(),\n",
    "        'opt_gen': opt_gen.state_dict(), 'opt_disc': opt_disc.state_dict(),\n",
    "        'epoch': epoch, 'step': step, 'alpha': alpha,\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "def load_checkpoint(gen, disc, opt_gen, opt_disc, path, device):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"âœ… Resuming training from checkpoint: {path}\")\n",
    "        state = torch.load(path, map_location=device)\n",
    "        gen.load_state_dict(state['gen'])\n",
    "        disc.load_state_dict(state['disc'])\n",
    "        opt_gen.load_state_dict(state['opt_gen'])\n",
    "        opt_disc.load_state_dict(state['opt_disc'])\n",
    "        return state['epoch'], state['step'], state['alpha']\n",
    "    print(\"ğŸ“­ No checkpoint found, starting from scratch.\")\n",
    "    return 0, 0, 1.0\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "### ROBUSTNESS FIX: New Clinical Loss ###\n",
    "# ----------------------------------------------------\n",
    "def calculate_clinical_loss(fake_masks, config):\n",
    "    \"\"\"\n",
    "    Calculates the L_clinical loss based.\n",
    "    This version implements L_area.\n",
    "    \"\"\"\n",
    "    if config[\"CLINICAL_LOSS_TYPE\"] == \"area\":\n",
    "        # fake_masks are logits (B, 1, H, W)\n",
    "        fake_masks_prob = torch.sigmoid(fake_masks)\n",
    "\n",
    "        # Calculate the area of the mask as a percentage of total pixels\n",
    "        # (B, 1, H, W) -> (B,)\n",
    "        total_pixels = fake_masks_prob.shape[2] * fake_masks_prob.shape[3]\n",
    "        mask_area_percent = fake_masks_prob.sum(dim=[1, 2, 3]) / total_pixels\n",
    "\n",
    "        # Get target plausible area stats\n",
    "        mean_area = config[\"PLAUSIBLE_LUNG_AREA_MEAN\"]\n",
    "        std_area = config[\"PLAUSIBLE_LUNG_AREA_STD\"]\n",
    "\n",
    "        # Calculate L1 distance from the mean, normalized by std\n",
    "        # This penalizes masks that are too large or too small\n",
    "        area_loss = F.l1_loss(mask_area_percent, torch.tensor(mean_area, device=mask_area_percent.device)) / std_area\n",
    "        return area_loss\n",
    "    else:\n",
    "        # Placeholder for L_contiguity or L_shape\n",
    "        return torch.tensor(0.0, device=fake_masks.device)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# --- ğŸ’¡ NEW UTILITY LOSS FUNCTION (FROM EVALUATION FEEDBACK) ---\n",
    "# --------------------------------------------------------------\n",
    "def calculate_diag_utility_loss(fake_images_512, cdiag):\n",
    "    \"\"\"\n",
    "    Calculates the utility loss.\n",
    "    This pushes the generator to create images that Cdiag sees as 'Pneumonia'.\n",
    "    \"\"\"\n",
    "    # Get predictions (logits) from Cdiag for the *in-batch* fakes\n",
    "    diag_preds_logits = cdiag(fake_images_512).squeeze()\n",
    "\n",
    "    # Create target labels of all '1.0' (Pneumonia)\n",
    "    target_labels = torch.ones_like(diag_preds_logits)\n",
    "\n",
    "    # Calculate the loss: how far are the predictions from 1.0?\n",
    "    loss = F.binary_cross_entropy_with_logits(diag_preds_logits, target_labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# --- ğŸ’¡ UPGRADED FAIRNESS LOSS (FROM EVALUATION FEEDBACK) ---\n",
    "# ----------------------------------------------------------------\n",
    "def calculate_fairness_loss(gen, cdiag, alpha, steps, config):\n",
    "    \"\"\"\n",
    "    Calculates a robust L_fairness.\n",
    "    Now combines two objectives:\n",
    "    1. loss_std: Make all TPRs EQUAL (std -> 0)\n",
    "    2. loss_push: Make all TPRs HIGH (mean -> 1.0)\n",
    "    \"\"\"\n",
    "    gen.eval() # Use eval mode for this calculation\n",
    "\n",
    "    B = config[\"FAIRNESS_BATCH_SIZE\"]\n",
    "    N_GROUPS = config[\"NUM_RACE_GROUPS\"]\n",
    "    DEVICE = config[\"DEVICE\"]\n",
    "\n",
    "    if B % N_GROUPS != 0:\n",
    "        raise ValueError(\"FAIRNESS_BATCH_SIZE must be divisible by NUM_RACE_GROUPS\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_fair = torch.randn(B, config[\"LATENT_DIM\"], device=DEVICE)\n",
    "        fake_images_fair = gen(z_fair, alpha, steps)\n",
    "        fake_images_512 = F.interpolate(fake_images_fair, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "\n",
    "    diag_preds_logits = cdiag(fake_images_512).squeeze()\n",
    "    diag_preds_probs = torch.sigmoid(diag_preds_logits)\n",
    "\n",
    "    samples_per_group = B // N_GROUPS\n",
    "    tpr_per_group = []\n",
    "    for j in range(N_GROUPS):\n",
    "        start_idx = j * samples_per_group\n",
    "        end_idx = (j + 1) * samples_per_group\n",
    "        tpr = diag_preds_probs[start_idx:end_idx].mean()\n",
    "        tpr_per_group.append(tpr)\n",
    "\n",
    "    if len(tpr_per_group) > 1:\n",
    "        tpr_stack = torch.stack(tpr_per_group)\n",
    "\n",
    "        # 1. Fairness (Equality) Loss: Make them equal\n",
    "        loss_std = torch.std(tpr_stack)\n",
    "\n",
    "        # 2. Utility (Push) Loss: Make them HIGH\n",
    "        # We want the mean TPR to be 1.0. Loss is (1.0 - mean_tpr)\n",
    "        loss_push = 1.0 - torch.mean(tpr_stack)\n",
    "\n",
    "        # 3. Combine them using the push weight\n",
    "        loss_g_fair = loss_std + config[\"FAIRNESS_PUSH_WEIGHT\"] * loss_push\n",
    "\n",
    "    else:\n",
    "        loss_g_fair = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "    gen.train() # Set generator back to train mode\n",
    "    return loss_g_fair\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop (MODIFIED)\n",
    "# ---------------------------\n",
    "def run_training():\n",
    "    DEVICE = CONFIG['DEVICE']\n",
    "    print(\"ğŸš€ Initializing Robust CAF-GAN Training...\")\n",
    "\n",
    "    # --- Models & Optimizers ---\n",
    "    gen = Generator(CONFIG['LATENT_DIM'], CONFIG['LATENT_DIM'], base_channels=512, img_channels=CONFIG['CHANNELS']).to(DEVICE)\n",
    "    disc = Discriminator(base_channels=512, img_channels=CONFIG['CHANNELS']).to(DEVICE)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=CONFIG['G_LR'], betas=(CONFIG['B1'], CONFIG['B2']))\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=CONFIG['D_LR'], betas=(CONFIG['B1'], CONFIG['B2']))\n",
    "\n",
    "    # --- Load Frozen Critics (Cdiag and Cseg) ---\n",
    "    Cdiag, Cseg = load_critics(CONFIG['CDIAG_PATH'], CONFIG['CSEG_PATH'], DEVICE)\n",
    "\n",
    "    # --- Checkpoint Loading ---\n",
    "    checkpoint_path = os.path.join(CONFIG['OUTPUT_DIR'], \"caf_gan_checkpoint.pth\")\n",
    "    start_epoch, start_step, alpha = load_checkpoint(gen, disc, opt_gen, opt_disc, checkpoint_path, DEVICE)\n",
    "\n",
    "    train_df = pd.read_csv(CONFIG['TRAIN_CSV_PATH'])\n",
    "    fixed_noise = torch.randn(8, CONFIG['LATENT_DIM'], device=DEVICE)\n",
    "\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    num_progressive_stages = len(CONFIG['PROGRESSIVE_EPOCHS'])\n",
    "\n",
    "    # --- Main Training Loop ---\n",
    "    for res_step in range(start_step, num_progressive_stages):\n",
    "        steps = res_step + 1\n",
    "        img_size = 4 * (2 ** steps)\n",
    "        if img_size > CONFIG['TARGET_IMG_SIZE']: break\n",
    "\n",
    "        loader_batch_size = CONFIG['BATCH_SIZES'][res_step]\n",
    "        num_epochs_for_res = CONFIG['PROGRESSIVE_EPOCHS'][res_step]\n",
    "\n",
    "        epoch_offset = start_epoch if res_step == start_step else 0\n",
    "        if epoch_offset > 0:\n",
    "            print(f\"Resuming at Epoch {epoch_offset+1}/{num_epochs_for_res} for this stage.\")\n",
    "        start_epoch = 0\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"ğŸ“ˆ Starting Training for Resolution: {img_size}x{img_size}\")\n",
    "        print(f\"   Epochs for this stage: {num_epochs_for_res} | Batch Size: {loader_batch_size}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        transform = A.Compose([\n",
    "            A.Resize(width=img_size, height=img_size, interpolation=Image.LANCZOS),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        dataset = MIMIC_GAN_Dataset(train_df, CONFIG['IMAGE_DIR'], transform=transform)\n",
    "        loader = DataLoader(dataset, batch_size=loader_batch_size, shuffle=True, num_workers=CONFIG['NUM_WORKERS'], pin_memory=True, collate_fn=custom_collate)\n",
    "\n",
    "        for epoch in range(epoch_offset, num_epochs_for_res):\n",
    "            current_epoch_total = sum(CONFIG['PROGRESSIVE_EPOCHS'][:res_step]) + epoch + 1\n",
    "            print(f\"\\n--- Resolution {img_size}x{img_size} | Epoch {epoch+1}/{num_epochs_for_res} (Total: {current_epoch_total}) ---\")\n",
    "\n",
    "            # --- Alpha and Lambda Annealing ---\n",
    "            if num_epochs_for_res > 1:\n",
    "                alpha = min(1.0, (epoch + 1) / (num_epochs_for_res * 0.5))\n",
    "            else:\n",
    "                alpha = 1.0\n",
    "\n",
    "            clinical_fair_lambda_scale = 1.0\n",
    "            if epoch < CONFIG['LOSS_ANNEAL_START_EPOCH']:\n",
    "                clinical_fair_lambda_scale = 0.0\n",
    "            elif epoch < num_epochs_for_res:\n",
    "                clinical_fair_lambda_scale = max(0.0, (epoch - CONFIG['LOSS_ANNEAL_START_EPOCH']) / max(1, (num_epochs_for_res - CONFIG['LOSS_ANNEAL_START_EPOCH'])))\n",
    "\n",
    "            loop = tqdm(loader, leave=True)\n",
    "            for batch_idx, (real, labels, races) in enumerate(loop):\n",
    "                if real.nelement() == 0:\n",
    "                    continue\n",
    "                real = real.to(DEVICE)\n",
    "\n",
    "                # ========== Train Discriminator ==========\n",
    "                noise = torch.randn(real.shape[0], CONFIG['LATENT_DIM'], device=DEVICE)\n",
    "                fake = gen(noise, alpha, steps)\n",
    "\n",
    "                disc_real = disc(real, alpha, steps)\n",
    "                disc_fake = disc(fake.detach(), alpha, steps)\n",
    "\n",
    "                # R1 Gradient Penalty\n",
    "                gp = 0.0\n",
    "                if batch_idx % CONFIG['R1_REG_FREQ'] == 0:\n",
    "                    real.requires_grad = True\n",
    "                    disc_real_for_gp = disc(real, alpha, steps)\n",
    "                    grad = torch.autograd.grad(outputs=disc_real_for_gp.sum(), inputs=real, create_graph=True)[0]\n",
    "                    grad_penalty = (grad.view(grad.shape[0], -1).norm(2, dim=1) ** 2).mean()\n",
    "                    gp = CONFIG['LAMBDA_R1'] * grad_penalty\n",
    "                    real.requires_grad = False\n",
    "\n",
    "                loss_disc = (torch.mean(F.softplus(disc_fake)) + torch.mean(F.softplus(-disc_real))) + gp\n",
    "\n",
    "                opt_disc.zero_grad()\n",
    "                loss_disc.backward()\n",
    "                opt_disc.step()\n",
    "\n",
    "                # ========== Train Generator ==========\n",
    "                gen_fake = gen(noise, alpha, steps)\n",
    "                disc_fake_for_g = disc(gen_fake, alpha, steps)\n",
    "                loss_g_adv = torch.mean(F.softplus(-disc_fake_for_g))\n",
    "\n",
    "                loss_g_clinic = torch.tensor(0.0, device=DEVICE)\n",
    "                loss_g_fair = torch.tensor(0.0, device=DEVICE)\n",
    "                # --- ğŸ’¡ NEW UTILITY LOSS VARIABLE ---\n",
    "                loss_g_utility = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "                # --- Apply Robust Clinical, Fair, and Utility Losses ---\n",
    "                if clinical_fair_lambda_scale > 0.0 and img_size == CONFIG['TARGET_IMG_SIZE']:\n",
    "\n",
    "                    fake_512 = F.interpolate(gen_fake, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "\n",
    "                    # --- 1. L_clinical (from Cseg) ---\n",
    "                    fake_masks = Cseg(fake_512)\n",
    "                    loss_g_clinic = calculate_clinical_loss(fake_masks, CONFIG)\n",
    "\n",
    "                    # --- 2. ğŸ’¡ NEW L_utility (from Cdiag) ---\n",
    "                    loss_g_utility = calculate_diag_utility_loss(fake_512, Cdiag)\n",
    "\n",
    "                    # --- 3. ğŸ’¡ UPGRADED L_fairness (from Cdiag) ---\n",
    "                    loss_g_fair = calculate_fairness_loss(gen, Cdiag, alpha, steps, CONFIG)\n",
    "\n",
    "\n",
    "                # --- ğŸ’¡ UPDATED Total Generator Loss ---\n",
    "                loss_gen = (\n",
    "                    loss_g_adv +\n",
    "                    clinical_fair_lambda_scale * CONFIG['LAMBDA_CLINIC'] * loss_g_clinic +\n",
    "                    clinical_fair_lambda_scale * CONFIG['LAMBDA_FAIR'] * loss_g_fair +\n",
    "                    clinical_fair_lambda_scale * CONFIG['LAMBDA_UTILITY'] * loss_g_utility\n",
    "                )\n",
    "\n",
    "                opt_gen.zero_grad()\n",
    "                loss_gen.backward()\n",
    "                opt_gen.step()\n",
    "\n",
    "                # --- ğŸ’¡ UPDATED TQDM Postfix ---\n",
    "                loop.set_postfix(\n",
    "                    D_loss=f\"{loss_disc.item():.4f}\",\n",
    "                    G_loss=f\"{loss_gen.item():.4f}\",\n",
    "                    G_adv=f\"{loss_g_adv.item():.4f}\",\n",
    "                    G_clinic=f\"{loss_g_clinic.item():.4f}\",\n",
    "                    G_fair=f\"{loss_g_fair.item():.4f}\",\n",
    "                    G_util=f\"{loss_g_utility.item():.4f}\",\n",
    "                    Alpha=f\"{alpha:.3f}\",\n",
    "                    Î»_Scale=f\"{clinical_fair_lambda_scale:.3f}\"\n",
    "                )\n",
    "\n",
    "                if batch_idx % CONFIG['CHECKPOINT_FREQ'] == 0:\n",
    "                    save_checkpoint(gen, disc, opt_gen, opt_disc, epoch, res_step, alpha, checkpoint_path)\n",
    "\n",
    "            # --- End of Epoch Actions ---\n",
    "            if (epoch + 1) % CONFIG['SAVE_IMG_FREQ'] == 0:\n",
    "                gen.eval()\n",
    "                with torch.no_grad():\n",
    "                    img_grid = gen(fixed_noise, alpha, steps).detach().cpu()\n",
    "                vutils.save_image(img_grid, os.path.join(CONFIG['IMAGE_OUTPUT_DIR'], f\"res_{img_size}_epoch_{current_epoch_total}.png\"), normalize=True, nrow=4)\n",
    "                print(f\"ğŸ–¼ï¸ Saved generated images for epoch {current_epoch_total}\")\n",
    "                gen.train()\n",
    "\n",
    "            save_checkpoint(gen, disc, opt_gen, opt_disc, epoch, res_step, alpha, checkpoint_path)\n",
    "            print(f\"ğŸ’¾ End of epoch {current_epoch_total}. Checkpoint saved.\")\n",
    "\n",
    "    print(\"\\nğŸ‰ğŸ‰ğŸ‰ CAF-GAN Training Complete! ğŸ‰ğŸ‰ğŸ‰\")\n",
    "    torch.save(gen.state_dict(), os.path.join(CONFIG['OUTPUT_DIR'], \"caf_gan_generator_final.pth\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
